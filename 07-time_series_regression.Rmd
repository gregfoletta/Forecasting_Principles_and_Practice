---
title: "Chapter 7 - Time Series Regression Models"
author: "Greg Foletta"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    toc: true
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

# Switch devices to allow for transparency..
knitr::opts_chunk$set(dev.args = list(png = list(type = "cairo")))
```

```{r, message = FALSE, include = FALSE}
library(tidyverse)
library(forecast)
library(fma)
library(magrittr)
library(fpp3)
library(tsibble)
library(lubridate)
library(feasts)
```

Basic concept is the forecasting the time series of interest $y$ assuming it has a linear relationship with other time series $x$.

# Simple Linear Regression

Standard model:

$$ y_t = \beta_0 + \beta_1X_t + \epsilon_t $$

```{r}
us_change %>% 
    ggplot(aes(x = Quarter)) +
    geom_line(aes(y = Consumption), colour = 'red') +
    geom_line(aes(y = Income), colour = 'blue') + 
    labs(
        x = 'Year',
        y = '% Change',
        title = 'US Personal Income and Consumption'
    )
```

We can show a scatter plot and an estimated linear fit:

```{r}
us_change %>% 
    ggplot(aes(Income, Consumption)) +
    geom_point() +
    geom_smooth(method = 'lm') +
    labs(
        x = 'Income (Qtr % Change)',
        y = 'Consumption (Qtr % Change)',
        title = 'US Income vs Consumption'
    )
```

The fit can be calculated using the TSLM (time series linear model) function:

```{r, comment = ''}
us_change %>% 
    model(lm = TSLM(Consumption ~ Income)) %>% 
    tidy()
```

The slope cas a coefficient of .27, meaning that for every 1% change in income, consumption goes up by .27%.

The intercept is when $x = 0$, so when there is no change in income, there is an increase in consumptiojn of .54%.

# Multiple Linear Regression

When there are two ro more predictor variables, the model is a **multiple regression**. It is of the form:

$$ y_t = \beta_0 + \beta_1 x_{1,t} + \beta_2 x_{2,t} + \ldots + \beta_p x_{p,t} + \epsilon_t $$

The coefficients measure the effect of each predictor after taking into account the effects of all other predictors - they measure the **marginal effects** of the predictor variables.

Below we look at three predictors that may be able to predict US consumption expenditure:

```{r}
us_change %>% 
    pivot_longer(
        c(Production, Savings, Unemployment), 
        names_to = 'Measure',
        values_to = 'Percentage'
    ) %>% 
    ggplot() +
    geom_line(aes(Quarter, Percentage, colour = Measure)) +
    facet_grid(rows = vars(Measure), scales = 'free') +
    labs(
        x = 'Quarter',
        y = '% Change',
        title = 'US Personal Fiscal Data',
        subtitle = 'Consumption, Savings, Unemployment'
    )
```

We can look at a scatterplot of the five variables:

```{r}
us_change %>% 
    GGally::ggpairs(columns = 2:6, progress = FALSE)
```

# Linear Model Assumptions

When fitting linear model, the following assumptions are made about the errors ($\epsilon_t$):

- They have a mean zero, otherwise the forecasts witll be biased.
- The are not correlated.
- They are unrelated to the predictor variables.

It is also useful that they are normally distributed with a constant variance. This allows us to produce prediction intervals.

# Least Squares Estimation

The least squares principle is a way of choosing the coefficients by minimising the sum or the squared errors. The $\beta_p$ are chosen that minimise:

$$ \sum_{t=1}^T \epsilon_t^2 = \sum_{t=1}^t (y_t - \beta_0 - \beta_1 x_{1,t} - \ldots - \beta_p x_{p,t})^2 $$

The `TSLM()` function fits a linear regression to time series data, similar to `lm()`.

```{r, comment = ''}
us_consumption_mdl <-
    us_change %>% 
    model(
        lm = TSLM(Consumption ~ Income + Production + Unemployment + Savings)
    )

us_consumption_mdl %>% 
    report()
```

# Fitted Values

Predictions for a data point can be calculated by simply plugging the $x_{p,t}$ values into the formula with the coefficients.

```{r}
us_consumption_mdl %>% 
    augment() %>% 
    rename(Fitted = .fitted) %>% 
    pivot_longer(c(Consumption, Fitted), names_to = 'Measure', values_to = 'Value') %>% 
    ggplot() +
    geom_line(aes(Quarter, Value, colour = Measure)) +
    labs(
        x = 'Quarter',
        y = '% Change',
        title = 'US Consumption',
        subtitle = 'Actual versus Model Fitted'
    )
```


# Goodness-of-Fit

## R-Squared

$R^2$ is the most common way to summarise how well a linear regression model fits the data.

$$ R^2 = \frac{
    \sum(\hat{y}_t - \bar{y})^2
}{
    \sum(y_t - \bar{y})^2
}$$

It is the proportion of the variation in the forecast accounted for (or explained) by the model.

$R^2$ will never decrease when adding in additional predictors, which can lead to over-fitting.

## Standard Error of the Regression

Another measure is the standard deviation of the residuals, or **residual standard error**.

$$ \hat{\sigma}_e = \sqrt{ \frac{1}{T - p - 1} \sum_{t=1}^T e^2_t } $$

Where $p$ is the number of predictors in the model. We take $p - 1$ away because we have estimated the intercept plus $p$ parameters.

The standard error is related to the size of the average error that the model produces. We compare this to the sample mean of $y$ or the standard deviation of $y$ to gain perspective on the accuracy of the model.

# Evaluating the Regression Model

The residuals are defined as $e_t = y_t - \hat{y}_t$. They also have other useful properties:

$$ \sum_{t=1}^t e_t = 0 \text{ and } \sum_{t=1}^Tx_{p,t} e_t = 0 \text{ for all } p $$

as a result, the mean of the residuals is zero, and the correlation between the residuals and the observations for the predictor variable is also zero.

## ACF Residual Plot

With time series data, it's very unlikely that a value of a variable in the current time period will be the same as the previous period. Thus it's common to find autocorrelation when fitting a regression model to time series data, and one of our assumptions is violated. 

What this means is that there's some information that hasn't been accounted for, and the forecasts aren't as efficient as they should be. The forecasts are still *unbiased*, and not wrong, but the will have a larger prediction interval than they need to.

The `gg_tsresiduals()` gives the three key diagnostic plots needed:

```{r}
us_consumption_mdl %>% gg_tsresiduals()
```

We see some chage in variation over time, but otherwise the residual plot looks reasonable. The heteroscedacticity makes the prediction intervals inaccurate. The residuals look "normal-ish", however they are skewed, which will also affect the prediction intervals.

## Residual Predictor Plots

We expect that the residuals should be randomly scattered without showing any systematic patterns. We can check this by plotting them against the predictors.

In the plots below, we don't see any noticible patterns between the residuals and the predictor values.

```{r}
us_change %>% 
    left_join(residuals(us_consumption_mdl), by = 'Quarter') %>% 
    pivot_longer(
        c(Income, Production, Savings, Unemployment),
        names_to = 'Economic_Measure',
        values_to = 'Economic_Value'
    ) %>% 
    ggplot() +
    geom_point(aes(Economic_Value, .resid)) +
    facet_wrap(vars(Economic_Measure), scales = 'free') +
    labs(
        x = 'Economic Measure %',
        y = 'Model Residuals',
        title = 'US Consumption',
        subtitle = 'Linear Model - Predictors versis Residuals'
    )
```

## Residual Versus Fitted

A plot of the residuals versus the fitted values should also show no pattern. If there is a pattern, the errors may be heteroscedastic, whereby the variance of the residuals is not constant.

From the plot below we see that the errors appear to be random, suggesting that the errors are homoscedastic.

```{r}
us_consumption_mdl %>% 
    augment() %>% 
    ggplot() +
    geom_point(aes(.fitted, .resid)) +
    labs(
        x = 'Fitted Consumption',
        y = 'Residuals',
        title = 'US Consumption Model',
        subtitle = 'Linear Model - Fitted versus Residuals'
    )
```

## Outliers and Influential Observations

Observation with extreme values are **outliers**, and observations that have a large influence on the coefficients of a regression  model are called **influential observatons** and these impart **leverage**. Usually, but not necessarily, outliers impart leverage.

There is no rigid mathematical definition of an outlier - it's ultimately subjective and depends on the context of the model. Observations can be flagged based on interquartile ranges - e.g. if $Q_1$ and $Q_3$ are the lower and upper quartiles, an outlier could be anything outside the range $[Q_1 - k(Q_4 - Q_1), Q_3 + k(Q_3 - Q_1)]$ for some non-negative constant $k$. John Tukey proposed $k = 1.5$.

Leverage is usually noted as $h_i$ for the $i$th observation, which can be found in the [hat matrixhttps://stats.stackexchange.com/questions/208242/hat-matrix-and-leverages-in-classical-multiple-regression) $\bf{H}_{ii}$.

## Spurious Regression

Generally, time series data are 'non-stationary', which means that the values of the series do not fluctuate around a constant mean or with constant variance. 

For example, let's regress production of cement with productio of electricity:

```{r}
aus_production %>% 
    ggplot(aes(Electricity, Cement)) +
    geom_point() +
    geom_smooth(method = 'lm', formula = 'y ~ x') +
    labs(
        title = 'Spurious Regression',
        subtitle = 'Electricity vs Cement Production'
    )
```

```{r, comment = ''}
aus_production %>% 
    model(
        lm = TSLM(Cement ~ Electricity)
    ) %>% 
    report()
```

We see that we get an $R^2$ of .89, indicating that 89% of the variation in the Cement response can be accounted for by the Electricity predictor. Of course the production of Cement has nothing to do with the production of Electricity, a classic case of 'correlation not causation'. What we have is a confounding variable or varibales, likely gross domestic product or population, which has a direct impact on both of these variables.

# Useful Predictors

## Trend

It's common for time series data to be trending. A linear trend can be modelled by using $x_{1,t} = t$ as a predictor, where $t = 1,\ldots,T$. A trend variable can be specified in the $TSLM()$ function using the $trend()$ special.

## Dummy Variables

Dummy variables are used within a linear regression to model caegorical variables. Can also be used for an outlier. If there are more than two categories, multiple variables can be used. You will use one less variables that there are categories, as the intercept will capture that variables when all the variables are set to zero.

The `TSLM()` will automatically handle this if the `season()` special is inlcuded.

Below we fit a linear model to Australian electricity demand.

```{r}
aus_electricity_lms <-
    aus_production %>%
    filter_index('1992' ~ .) %>% 
    model(
        `Trend LM` = TSLM(Electricity ~ trend()),
        `Trend + Season LM` = TSLM(Electricity ~ trend() + season())
    )

aus_electricity_lms %>% 
    select(`Trend + Season LM`) %>% 
    report()
```

So we can see that, for every quarter, ~280 Gigawatt hours more electricity is produced. On average the second quarter has ~990 Gwh more elctricity produced, and quarter 3 has 2506Gwh more electricity - which lines up with the majority of the summer season.

```{r}
aus_electricity_lms %>% 
    augment() %>% 
    ggplot(aes(x = Quarter)) +
    geom_line(aes(y = Electricity, colour = 'Real Values')) +
    geom_line(aes(y = .fitted, colour = .model)) +
    labs(
        colour = 'Data Source',
        title = 'Australian Electricity Demand',
        subtitle = 'Linear Regressions',
        x = 'Year',
        y = 'Gigawatt Hours Produced'
    )
```

```{r}
aus_electricity_lms %>%
    select(`Trend + Season LM`) %>% 
    augment() %>% 
    ggplot() +
    geom_point(aes(Electricity, .fitted, colour = as.factor(quarter(Quarter)))) +
    geom_abline(slope = 1, intercept = 0) +
    scale_colour_brewer(palette="Dark2") +
    labs(
        title = 'Australian Electricity Demand',
        subtitle = 'Linear Regressions - Response vs. Fitted',
        x = 'Electricity Demand (GWh)',
        y = 'Linear Regression Fitted Values',
        colour = 'Quarter'
    )
```

## Intervention Variables

It can be necessary to omdel interventions that may affect the response variable - e.g. competitor activity, advertising expenditure, industrial action, etc.

When the action only lasts for one quarter, a 'spkike' variable is used which takes the value of one in the period of the action.

If the action changes the the values suddenly and permanently, a 'step' variable is used, taking the value 0 before the action and 1 after.

Piecewise linear models can be used where the trend change at an inflection point.

## Trading Days

The number of trading days in a month can vary substantially, and can be added in as a predictor.

## Distributed Lags

It can be useful to include advertising expenditure, however as this can last beyond the actual campaign, lagged values of the expenditure need to be included:

$$
x_1 = \text{ advertising for previous month.} \\
x_2 = \text{ advertising for previous two months.} \\
\vdots \\
x_m = \text{ advertising for previous } m { months.}
$$

The coefficients should decrease as the lag increases.

## Easter

Easter differs from most holidays because it is not held on the same date wach year, and its effects can last several days. With monthly data, if easter falls in March then the dummy variable takes value 1 in March. If Easter is over two months, the dummy variable is split proportionally across the months.

## Fourier Series

An alternative to dummy variables, especially for long seasonal periods, is to use Fourier terms. With a Fourier series, a set of sine and cosine functions can approximate any periodic function.

If $m$ is the seasonal period, then the first few Fourier terms are:

$$
x_{1,t} = sin\bigg(\frac{2 \pi t}{m}\bigg), x_{2,t} = con\bigg(\frac{2 \pi t}{m}\bigg) \\
x_{3,t} = sin\bigg(\frac{4 \pi t}{m}\bigg), x_{4,t} = cos\bigg(\frac{4 \pi t}{m}\bigg) \\
x_{5,t} = sin\bigg(\frac{6 \pi t}{m}\bigg), x_{6,t} = cos\bigg(\frac{6 \pi t}{m}\bigg) \\
$$

If there is monthly seasonality, and we use the first 11 of these predictor variables, it will be the same forecasts as using 11 dummy variables. The benefit is that fewer predictors are required than with dummy variables. This is useful for weekly data where $m \approx 52$.

These terms can be produced using the `fourier()` special. The $K$ argument specifies how many pairs of $sin()$ and $cos()$ pairs to include. The maximum allowed is $K = m/2$.

If only the first two are used, the seasonal pattern is a simple sine wave. A regression with Fourier terms is often called a **harmonic regression**.

```{r}
aus_production %>% 
    model(TSLM(Beer ~ trend() + fourier(K = 2))) %>% 
    augment() %>% 
    ggplot() +
    geom_line(aes(Quarter, Beer, colour = 'Beer')) +
    geom_line(aes(Quarter, .fitted, colour = 'Fitted')) +
    labs(
        colour = 'Data Source',
        title = 'Australian Beer Production',
        subtitle = 'Linear Regression with Trend & Fourier Terms',
        x = 'Year',
        y = 'Megalitres'
    )
        
```

# Selecting Predictors

Common approaches that are *not recommended* are:

- Plotting the response against a particular predictor and visually determining if there is a relationship.
- Perform multiple linear regression on all predictors and disregard based on p-value.
    - Statistical significance does not always indicate predictive value.
    - P-values are misleading with correlated predictors.
    
Instead, the following measures can be used:

## Adjusted R$^2$

- Not a good measure of predictive ability.
- Does not allow for degrees of freedom: adding any variable will increase its value.
- Minimusing sum of squared errors (SSE) is equivalent.

An alternative is adjusted R^2, which is equivalent to minimising the standard error, and doesn't increase with each added predictor:

$$ \bar{R}^2 = 1 - (1 - R^2) \frac{T - 1}{T - k - 1} $$
Where $T$ is the number of observations and $k$ is the number of predictors.

## Cross-Validation

- Can use classical leave-one-out cross validation (LOOCV):

1. Remove observation $t$.
1. Fit the model using the remaining data.
1. Predict the value of $t$ and compute the error $e_t^* = y_t - \hat{y}_t$.
1. Repeat step for $t, \ldots, T$.
1. Compute the MSE from $e_1^*, \ldots, e_T^*$.


## Akaike's Information Criterion

$$ AIC = T log\bigg(\frac{SSE}{T}\bigg) + 2(k + 2) $$

The $k + 2$ part of the equation occurs because there are that many parameters in the model: the $k$ predictors, the intercept, and the variance of the residuals.

The model with the minimum AIC is often the best model for forecasting.

## Corrected AIC

For small values of $T$, the AIC tends to select too many predictors. A bias-corrected version  has been developed:

$$ AIC_c = AIC + \frac{ 2(k+2)(k+3)}{T - k - 3} $$

## Schwarz's Bayesian Information Criterion

Usually abbreviated to BIC, SBIC or SC, this value should be minimised. BIC penalises the number of parameters more heavily than AIC. For large values of $T$, it is similar to leave-$v$-out cross validation, where $v = T(1 - \frac{1}{log(T) - 1})$

$$ BIC = T log\bigg(\frac{SSE}{T}\bigg) + (k+2)log(T) $$

## Which One?

- $\bar{R}^2$ is widely used, but has a tendency to select too many predictors.
- BIC is liked because if there is a ture underlying model, BIC will select it given enough data.
    - However there is rarely a true underlying model.
    - Selecting that model may not necessarily give the best forecasts.
    
Hyndman and Athanasopoulos recommend one of $AIC_c$, $AIC$ or $CV$ be used.

